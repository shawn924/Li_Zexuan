---
title: "Assignment 3: Supervised learning competition"
author: 
  - Ilse van Deventer (9996974)
  - Majdouline Hamdi (9767738)
  - Zexuan Li (8069182)
  - Zoé Ricardie (9107096)
  - Menno Zoetbrood (1084720)
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R Packages
#| echo: false
#| warning: false
#| message: false

# Loading packages

# Handling data
library(tidyverse)
library(dplyr)
library(data.table)
library(Matrix)
library(reshape2)

# Visualisation
library(ggplot2)
library(GGally)
library(heatmaply)
library(RColorBrewer)
library(corrplot)

# Statistical Analysis
library(parameters)
library(psych)
library(MASS)
library(broom)

# Machine Learning
library(caret)
library(randomForest)
```


```{r setup, include=FALSE}
# Ignoring warning
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)
```

```{r}
#| label: Data Loading
#| echo: false

test_data <- readRDS("test.rds")  # Test data which does not include the target variable
train_data <- readRDS("train.rds") # Data including the target variable (score)
```

## Data Description

This report aims to predict student academic performance using datasets collected from two schools. The available data include a variety of social factors (e.g. parental education, family income), and academic factors (e.g. study time, participation in extracurricular activities). These attributes act as potential predictors of the variable `score` and allow for holistic data analysis of how social and educational factors influence student performance (see Table 1).

Previous research shows the significant role social factors, such as socio-economic status, play in student performance (e.g. Rakesh et al., 2024; Munir et al., 2023). Thus, the dataset provides sufficient and diverse features to justify a comprehensive predictive analysis.

```{r}
#| warning: false
#| label: Create Table Overview Variables


# Create metadata table
var_info <- tribble(
  ~No, ~Variable, ~Description, ~Type, ~Possible_Values,
  1, "school", "Student’s school", "factor", '"GP" – Gabriel Pereira, "MS" – Mousinho da Silveira',
  2, "sex", "Student’s sex", "factor", '"F" – female, "M" – male',
  3, "age", "Student’s age", "numeric", "15–22",
  4, "address", "Student’s home address type", "factor", '"U" – urban, "R" – rural',
  5, "famsize", "Family size", "factor", '"LE3" – ≤3, "GT3" – >3',
  6, "Pstatus", "Parents’ cohabitation status", "factor", '"T" – together, "A" – apart',
  7, "Medu", "Mother’s education level", "numeric", "0–4 (none–higher)",
  8, "Fedu", "Father’s education level", "numeric", "0–4 (none–higher)",
  9, "Mjob", "Mother’s occupation", "factor", '"teacher", "health", "services", "at_home", "other"',
  10, "Fjob", "Father’s occupation", "factor", '"teacher", "health", "services", "at_home", "other"',
  11, "reason", "Reason for choosing school", "factor", '"home", "reputation", "course", "other"',
  12, "guardian", "Student’s guardian", "factor", '"mother", "father", "other"',
  13, "traveltime", "Home–school travel time", "numeric", "1: <15 min., 2: 15 to 30 min., 3: 30 min. to 1 hour, or 4: >1 hour)",
  14, "studytime", "Weekly study time", "numeric", "1–4 (<2 h to >10 h)",
  15, "failures", "Number of past class failures", "numeric", "0–4",
  16, "schoolsup", "Extra educational support", "factor", "yes/no",
  17, "famsup", "Family educational support", "factor", "yes/no",
  18, "paid", "Extra paid classes", "factor", "yes/no",
  19, "activities", "Participation in extra-curricular activities", "factor", "yes/no",
  20, "nursery", "Attended nursery school", "factor", "yes/no",
  21, "higher", "Aspires to higher education", "factor", "yes/no",
  22, "internet", "Internet access at home", "factor", "yes/no",
  23, "romantic", "In a romantic relationship", "factor", "yes/no",
  24, "famrel", "Quality of family relationships", "numeric", "1–5 (very bad–excellent)",
  25, "freetime", "Free time after school", "numeric", "1–5 (very low–very high)",
  26, "goout", "Going out with friends", "numeric", "1–5 (very low–very high)",
  27, "Dalc", "Workday alcohol consumption", "numeric", "1–5 (very low–very high)",
  28, "Walc", "Weekend alcohol consumption", "numeric", "1–5 (very low–very high)",
  29, "health", "Current health status", "numeric", "1–5 (very bad–very good)",
  30, "absences", "Number of school absences", "numeric", "0–93",
  31, "score", "Computed performance score", "numeric", "Continuous value"
)


# Add table to quarto document 
knitr::kable(var_info, caption = "Table 1. Variable Descriptions")


```


## Exploratory Data Analysis
To explore and understand the data, we followed Peng’s Exploratory Data Analysis (EDA) Checklist (Komorowski et al., 2016), which emphasises a structured, reproducible approach to data familiarization and validation.

Although our goal is not to answer a specific research question, this framework ensures a systematic process for identifying which variables best predict a student’s score and evaluating different predictive models.

After loading the dataset, we examined the first and last rows to verify that it was read correctly. The data contains 395 total students. The data gets split into two datasets, a training dataset and a test dataset. The training dataset includes 276 students attending Gabriel Pereira (GP), and 40 attending Mousinho da Silveira (MS), having a total of 316 students. Furthermore, the training dataset contains 31 variables, of which 30 are predictors, with the other being our target variable `score`, representing a normalised exam grade. The test dataset contains the remaining 79 students, containing 30 variables, omitting the `score` variable. 

```{r}
#| label: EDA Visualisation
#| include: false

# Read into the data 
glimpse(train_data)

str(train_data)

# Look at the top and the bottom of your data
head(train_data)
tail(train_data)

# Check N in data
dim(train_data)

# Check class all variables variables 
sapply(train_data, class)

# Check missing data per variable 
colSums(is.na(train_data))


# Check numeric data
summary(train_data[,sapply(train_data,is.numeric)])
# Check factor data
summary(train_data[,sapply(train_data,is.factor)])
```

### Preprocessing for EDA

Before conducting the EDA, we performed a series of Preprocessing steps to make the dataset interpretable and ready for visualisation. Ordinal variables such as parental education (`Medu`, `Fedu`), study-time, and travel time were converted into labelled factors with meaningful descriptions. Binary variables (e.g. `romantic`, `higher`) were mapped to 0/1 format for consistency, and categorical variables were grouped into numeric, ordinal, nominal, and binary sets to support targeted plotting.

In addition, one-hot encoding was applied to the main categorical variables to allow for bar plot visualisation and maintain categorical integrity during correlation inspection. These transformations ensured that all EDA figures reflected the underlying data structure accurately and consistently.


```{r}

#| label: EDA Preprocessing
#| include: False

# Create a copy of train_data
train_data_pre <- train_data

# Transferring ordered ordinal into factors
edu_map <- c("none", "primary (4th grade)", "5th to 9th grade", "secondary", "higher education")
traveltime_map <- c("<15 min", "15–30 min", "30 min–1 hour", ">1 hour")
studytime_map <- c("<2 hours", "2–5 hours", "5–10 hours", ">10 hours")

train_data_pre$Medu <- factor(train_data_pre$Medu, levels = 0:4, labels = edu_map)
train_data_pre$Fedu <- factor(train_data_pre$Fedu, levels = 0:4, labels = edu_map)
train_data_pre$traveltime <- factor(train_data_pre$traveltime, levels = 1:4, labels = traveltime_map)
train_data_pre$studytime <- factor(train_data_pre$studytime, levels = 1:4, labels = studytime_map)

# Create vectors depend on data type
numeric_ordinal <- c("failures","famrel","freetime","goout","Dalc","Walc","health")

ordered_ordinal <- c("Medu","Fedu","traveltime","studytime")

nominal_cols <- c("Mjob","Fjob","reason","guardian")

binary_cols <- c("school","sex","address","famsize","Pstatus",
                 "schoolsup","famsup","paid","activities","nursery",
                 "higher","internet","romantic")

numeric_cols <- c("absences", "age","score")

# Transfer numeric_ordinal into factor
train_data_pre[numeric_ordinal] <- lapply(train_data_pre[numeric_ordinal], as.factor)


#---

# Making another copy for train data for encoding
train_data_encode<-train_data_pre

# Transfer Binary into 0/1
binary_mapping <- list(
  school = c("GP" = 1, "MS" = 0),
  sex = c("F" = 1, "M" = 0), 
  address = c("U" = 1, "R" = 0),
  famsize = c("LE3" = 1, "GT3" = 0),
  Pstatus = c("T" = 1, "A" = 0),
  schoolsup = c("yes" = 1, "no" = 0),
  famsup = c("yes" = 1, "no" = 0),
  paid = c("yes" = 1, "no" = 0),
  activities = c("yes" = 1, "no" = 0),
  nursery = c("yes" = 1, "no" = 0),
  higher = c("yes" = 1, "no" = 0),
  internet = c("yes" = 1, "no" = 0),
  romantic = c("yes" = 1, "no" = 0)
)

for(col in names(binary_mapping)) {
  train_data_encode[[col]] <- as.numeric(binary_mapping[[col]][train_data_encode[[col]]])
}

# One-Hot encoding
dummy_model <- dummyVars(~ Medu + Fedu + traveltime + studytime + Mjob + Fjob + reason + guardian, 
                        data = train_data_encode, 
                        fullRank = FALSE) 

dummy_data <- predict(dummy_model, newdata = train_data_encode)

train_data_encode <- cbind(
  train_data_encode %>% dplyr::select(-Medu, -Fedu, -traveltime, -studytime, -Mjob, -Fjob, -reason, -guardian),
  as.data.frame(dummy_data)
)

```


`Score` was already normalised, with a mean of −0.02 (SD = 0.99), indicating a near-zero mean and unit variance. Therefore, no additional transformation or scaling was required for the dependent variable.


```{r}

#| label: Mean and Std. Dev. for Score

# Compute statistic score for data

stat_score <- train_data %>%
  summarize(mean(score), 
            sd (score))

stat_score
```

### Visualisation

##### Univariate Analysis

To better understand the data, we created five different bar plots, grouped by variable type. This was an intentional decision to keep each figure remaining clear, interpretable, and directly linked to their analytical role:
Figure 1 shows the ordered numeric variables, such as alcohol consumption (`Dalc`, `Walc`) and free time. The distributions highlight that most students report low alcohol consumption and good family relationships, with health and social activity levels being more varied.

Figure 2 displays ordered categorical variables, including parental education (`Fedu`, `Medu`), and travel time. These are ordinal by nature as they provide a meaningful order. Therefore, separating them into their own plot allows us to interpret trends such as the vast amount of parents with secondary or higher education, and the majority of students having their travel time to school being less than fifteen minutes.

Figure 3 focuses on the nominal variables specifically. Each bar plot displays the frequency of categories within these variables. The distribution shows that “services” and “other” are the most common parental job types (`Fjob`, `Mjob`), “mother” being the most frequent guardian (`guardian`), and “reputation and “course” being the leading reasons for school selection (`reason`).

Figure 4 depicts the continuous numeric variables. The absences variable is noticeably right-skewed, indicating that most students attend regularly, with only a few exhibiting high absence counts. The age distribution shows that the majority of students are between 15 and 18 years old, while the score variable follows an approximately normal distribution, making it well-suited for regression-based modelling. Specifically, students’ ages range from 15 to 22 years (M = 16.75, SD = 1.30), providing sufficient variation to capture academic outcomes across both early and late adolescence.

Figure 5 displays all binary variables. The bar plots present relatively balanced proportions across most binary features. However, attributes such as, `higher`, and `internet`, are heavily skewed towards “yes”. These distributions are expected and reflect the real world, as most students pursue higher education (e.g. Yuen et al., 2023), and home internet access is nearly universal. Focusing on `sex`, Figure 5 shows that there are 173 females and 143 males, indicating a slightly higher proportion of females.

```{r}

#| label: Frequencies plot

# Visualise Ordered Numeric data
train_data_pre %>%
  dplyr::select(dplyr::all_of(numeric_ordinal)) %>%
  tidyr::gather() %>%
  ggplot(aes(value)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 1. Ordered Numeric Variables", x = "", y = "Count")

# Visualise Ordered Categorical data
train_data_pre %>%
  dplyr::select(dplyr::all_of(ordered_ordinal)) %>%
  tidyr::gather() %>%
  ggplot(aes(value)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 2. Ordered Categorical Variables", x = "", y = "")

# Visualise Nominal data
train_data_pre %>%
  dplyr::select(dplyr::all_of(nominal_cols)) %>%
  tidyr::gather() %>%
  ggplot(aes(value)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 3. Nominal Variables", x = "", y = "")

# Visualise Numeric data
train_data_pre %>%
  dplyr::select(dplyr::all_of(numeric_cols)) %>%
  tidyr::gather() %>%
  ggplot(aes(value)) + 
  geom_histogram(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 4. Numeric Variables", x = "", y = "")

# Visualise Binary data
train_data_pre %>%
  dplyr::select(dplyr::all_of(binary_cols)) %>%
  tidyr::gather() %>%
  ggplot(aes(value)) + 
  geom_bar(fill = "steelblue", alpha = 0.7) +
  facet_wrap(~key, scales = "free", ncol = 4) + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Binary Variables", x = "", y = "")
```


##### Bivariate Analysis

###### Correlation Analysis

To examine how numerical variables relate to one another and to the target variable score, we generated a correlation heatmap (see Figure 6). This visualisation provides an overview of linear relationships between predictors and helps identify potential multicollinearity issues that could affect model performance. 

Figure 6 shows that most correlations are relatively weak, implying that the data does not contain strong redundancy between predictors. Additionally, our strongest positive correlations are between mother’s education `Medu` and father’s education `Fedu` (r = 0.61), and between weekday alcohol consumption `Dalc` and weekend alcohol consumption `Walc` (r = 0.63). The correlation for education level is expected as parental education levels tend to be similar within households (Erola et al., 2016), and for alcohol consumption, it indicates a consistent drinking pattern across the week. 

Thus, the heatmap provides a concise summary of inter-variable dependencies, affirming that, while some variables are relatively interrelated, most predictors provide unique information, making their inclusion in the following modeling steps sufficient.

Focusing on `score`,  most variables show little correlation. Notably, the strongest negative correlation with `score` is a student’s past class failures (r = -0.41). So, students who have failed previous classes tend to achieve lower exam scores, a normal correlation. Other notable negative correlation include `age` (r = -0.17) and `goout` (r = 0.18). On the contrary, both of the parents education level showed the highest positive correlation with `score` (`Medu` (r = 0.23), `Femu` (r = 0.19), with `studytime` being third (r = 0.12). This suggests that the education levels of parents is more important than the amount of time a student studies.
```{r}

#| label: Correlation Heatmap


# Calculate correlation matrix only with numeric columns from original data
corr <- cor(train_data[,sapply(train_data, is.numeric)], use = "complete.obs")

# Create correlation heatmap
corrplot(corr, 
         method = "color",
         type = "upper",
         order = "original",
         diag = FALSE,
         tl.cex = 0.8,
         tl.col = "black",
         number.cex = 0.7,
         addCoef.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(100),
         mar = c(0, 0, 2, 0))

title("Figure 6. Correlation Heatmap of Numerical Features", line = 3)

```


###### Data Exploration and Variable Relationships

Having examined the numeric variables and their correlations through the heatmap (Figure 6), we now turn to how categorical predictors relate to students’ exam performance.

Before delving into categorical analyses, we briefly inspected the distributions of numeric variables using boxplots. While most features showed expected variation, a few outliers were observed. Notably, unusually high values for absences (Figure 13) and a small number of older students relative to the rest of the cohort (Figure 8). As these observations appeared to reflect genuine cases rather than data entry errors, they were retained in the dataset.

We also analysed how the categorical variables of our dataset correlate with students' exam scores.

###### Demographics

Figure 7 shows the distribution of scores by sex. Males have slighly higher median scores than females, but there is considerable variability within both groups. 

```{r}

#|: label: Demographics


# Visualise boxplot by score by sex 
ggplot(train_data, aes(x = sex, y = score)) +
  geom_boxplot(outlier.colour = 'red') +
  scale_x_discrete(labels = c("F" = "Female", "M" = "Male")) +
  labs( x = "Sex", y = "Score",  title = "Figure 7. Score by Student Sex") +
  theme_minimal() 


# Visualise score by age 
ggplot(train_data, aes(age, score)) +
  geom_point() +
  geom_smooth(method = "lm") + # Score and age negative relation, seems linear. 
  theme_minimal()+
  labs( x = "Age", y = "Score",  title = "Figure 8. relation Score and age training data ") 
```

###### General Home Situation 

Next, we explored variables describing students’ general home environment, which are not directly related to school factors. Figure 10 compares performance between students from urban and rural areas, as well as those from small versus large families. Students whose parents live together appear to perform slightly better than those whose parents live apart. Furthermore, for the guardian variable, students whose main guardian is listed as “other” tend to achieve lower scores than those whose main guardian is either their mother or father.

```{r}

#| label: Visual Home Situation (General)

# Create vectors for split visual num & cat. 
num_home_general <- c("traveltime", "famrel")
cat_home_general <- c("address", "famsize", "Pstatus", "guardian")


# Recode categorical variables labels for readability
train_data_plot <- train_data |>
  mutate(
    address = recode(address, "U" = "Urban", "R" = "Rural"),
    famsize = recode(famsize, "LE3" = "≤3", "GT3" = ">3"),
    Pstatus = recode(Pstatus, "T" = "Together", "A" = "Apart"))


# Make vector for titles plot 
understandable_labels_plot_home_situ <- c(
  address = "Home Address",
  famsize = "Family Size",
  Pstatus = "Parents' Cohabitation",
  guardian = "Guardian",
  traveltime = "Travel Time to School",
  famrel = "Family Relationship Quality")

# Numeric plot with nice facet labels
train_data_plot |>
  pivot_longer(all_of(num_home_general), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = factor(value), y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable,  ncol = 2, labeller = labeller(variable = understandable_labels_plot_home_situ)) +
  labs( title = "Figure 9. General Home Situation (Numeric) vs Score", x = "Value", y = "Score") +
  theme_minimal()

# Categorical plot with nice facet labels
train_data_plot |>
  pivot_longer(all_of(cat_home_general), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable,  scales = "free", labeller = labeller(variable = understandable_labels_plot_home_situ)) +
  labs(title = "Figure 10. General Home Situation (Categorical) vs Score",  x = "Category", y = "Score" ) +
  theme_minimal() 

```

###### Home Situation (School Related)

Following the general home environment, we analysed school-related home factors. Most notable, Figure 9 indicates that students with home internet access tend to achieve higher exam scores. Also, parents who have an occupation in the health, teaching, or services sector, see their children achieving slightly higher scores.

```{r}

#| label: Visual Home Situation (School Related)

num_home_school <- c("Medu", "Fedu")
cat_home_school <- c("Mjob", "Fjob", "famsup", "internet", "nursery")


# Make vector for titles plot 
nice_labels_school <- c(
  Medu = "Mother's Education",
  Fedu = "Father's Education",
  Mjob = "Mother's Job",
  Fjob = "Father's Job",
  famsup = "Family Educational Support",
  internet = "Internet Access",
  nursery = "Attended Nursery"
)

# Numeric plot
train_data |>
  pivot_longer(all_of(num_home_school), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = factor(value), y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_school)) +
  labs(title = "Figure 11.Home Situation - School Related (Numeric) vs Score", x = "Value", y = "Score") +
  theme_minimal()

# Categorical plot
train_data |>
  pivot_longer(all_of(cat_home_school), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_school)) +
  labs(title = "Figure 12. Home Situation - School Related (Categorical) vs Score", x = "Category", y = "Score") +
  theme_minimal() 



```



###### School Motivation

We then examined school motivation variables. Figure 15 suggests that students who chose their school for its courses or reputation, as well as those who aspire to pursue higher education, tend to achieve higher mean scores.

```{r}

#| label: School Motivation 

num_school_motivation <- c("absences")
ord_school_motivation <- c("studytime", "failures")
cat_school_motivation <- c("reason", "higher")


# Make vector for titles plot 
nice_labels_motivation <- c(
  absences = "Number of Absences",
  studytime = "Weekly Study Time",
  failures = "Past Class Failures",
  reason = "Reason for School Choice",
  higher = "Aspires to Higher Education"
)

# Numeric
train_data |>
  pivot_longer(all_of(num_school_motivation), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~variable, scales = "free")+
  labs(title = "Figure 13. School Motivation (Numeric) vs Score", x = "Value",y = "Score" ) +
  theme_minimal()

# Ordinal
train_data |>
  pivot_longer(all_of(ord_school_motivation), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = factor(value), y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_motivation)) +
  labs(title = "Figure 14. School Motivation (Ordinal) vs Score", x = "Value", y = "Score") +
  theme_minimal()

# Categorical
train_data |>
  pivot_longer(all_of(cat_school_motivation), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_motivation)) +
  labs(title = "Figure 15. School Motivation (Categorical) vs Score", x = "Category", y = "Score") +
  theme_minimal() 
```


###### Extra Support

Next, we explored academic support variables. Figure 16 shows that students who pay for extra classes generally perform better than those who do not, while those receiving extra educational support tend to have lower mean scores. This may reflect differences in motivation: students paying for extra classes might be more proactive learners, whereas those receiving additional support could be struggling academically.

```{r}

#| label: Description Extra Support 

cat_extra_support <- c("schoolsup", "paid")

# Make vector for titles plot 
nice_labels_extra <- c(schoolsup = "Extra Educational Support", paid = "Paid Classes")

# Plot variables extra support 
train_data |>
  pivot_longer(all_of(cat_extra_support), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", labeller = labeller(variable = nice_labels_extra)) +
  labs(title = "Figure 16. Extra Support vs Score", x = "Category", y = "Score") +
  theme_minimal() 
```


###### Check Free Time Variables

When examining lifestyle variables, Figure 18 shows that participation in extracurricular activities does not appear to influence exam performance significantly. However, students in romantic relationships tend to score slightly lower than those who are not.

```{r}

#| label: Check Free Time variables 


# Visualise ordinal variables 

num_free_time <- c("freetime", "goout", "Dalc", "Walc")
cat_free_time <- c("activities", "romantic")

# Named vector for descriptive facet labels
nice_labels_free_time <- c(
  freetime = "Free Time After School",
  goout = "Going Out With Friends",
  Dalc = "Workday Alcohol Use",
  Walc = "Weekend Alcohol Use",
  activities = "Extra-Curricular Activities",
  romantic = "Romantic Relationship Status"
)

# Numeric plot
train_data |>
  pivot_longer(all_of(num_free_time), names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = factor(value), y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_free_time)) +
  labs(title = "Figure 17. Free Time Variables (Numeric) vs Score", x = "Value", y = "Score") +
  theme_minimal()

# Categorical plot
train_data |>
  pivot_longer(all_of(cat_free_time), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_free_time)) +
  labs(title = "Figure 18. Free Time Variables (Categorical) vs Score", x = "Category", y = "Score") +
  theme_minimal() 
```


##### Other Predictors

We also explored whether the school a student attends affects their performance. Figure 20 shows that students from school GP have a higher mean score than those from school MS. However, the sample sizes are highly imbalanced (276 vs 40 students, respectively). Thus, we should interpret the mean difference cautiously — the smaller MS sample may not represent its broader student population accurately.

```{r}

#| label:  Other Predictors 

num_other <- c("health")
cat_other <- c("school")

# Named vector for descriptive facet labels
nice_labels_other <- c(
  health = "Current Health Status",
  school = "School")

# Recode categorical values for readability
train_data_plot <- train_data |>
  mutate(school = recode(school, "GP" = "Gabriel Pereira", "MS" = "Mousinho da Silveira"))

# Numeric plot
train_data_plot |>
  pivot_longer(all_of(num_other), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = factor(value), y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_other)) +
  labs(title = "Figure 19. Other Predictors (Numeric) vs Score", x = "Value", y = "Score") +
  theme_minimal()

# Categorical plot
train_data_plot |>
  pivot_longer(all_of(cat_other), names_to = "variable", values_to = "value") |>
  ggplot(aes(x = value, y = score)) +
  geom_boxplot(outlier.colour = "red") +
  facet_wrap(~variable, scales = "free", ncol = 2, labeller = labeller(variable = nice_labels_other)) +
  labs(title = "Figure 20. Other Predictors (Categorical) vs Score", x = "Category", y = "Score") +
  theme_minimal() 

```

###### Visualise Interactions


Finally, we examined the relationship between students’ exam scores and the education level of their main caregiver (Figure 21). To do this, we constructed a new variable, `year_edu_main_caregiver`, representing the years of education completed by the student’s primary caregiver. This variable took the mother’s education level when the guardian was the mother, the father’s education level when the guardian was the father, and was coded 0 for “other” guardians.

When correlating `year_edu_main_caregiver` with exam scores, we found a weak relationship (ρ = 0.25), suggesting this feature added limited predictive value beyond existing parental education variables. Although this variable improved interpretability by directly linking the caregiver’s education to the student, it also introduced missing (zero-coded) values. This created challenges for k-fold cross-validation, which cannot efficiently handle incomplete data. Consequently, the variable was excluded from model training.


```{r}

#| label: Check Interaction (mom/dad/mean) Year Education & Main Guardian on Score

data_year_edu_parent <- train_data %>%
  dplyr::select(Medu, Fedu, score, guardian, famsup, famrel) %>%
  pivot_longer(
    cols = c(Medu, Fedu),
    names_to = "parent",
    values_to = "year_edu"
  )

ggplot(data_year_edu_parent, aes(x = year_edu, y = score, fill = parent)) +
  geom_boxplot() +
  facet_wrap(~ guardian) +
  labs(
    title = "Figure 21. Student Score vs Parental Education Level, by Guardian Type",
    x = "Years of Education",
    y = "Score",
    fill = "Parent"
  ) +
  theme_minimal() 

# Check main caregiver & year education 
data_year_edu_main_caregiver <- train_data |>
  mutate(
    year_edu_main_caregiver = case_when(
      guardian == "mother" ~ Medu,
      guardian == "father" ~ Fedu,
      guardian == "other"  ~ NA_real_))

cor_test_year_edu_main_care <- cor.test(data_year_edu_main_caregiver$year_edu_main_caregiver, data_year_edu_main_caregiver$score, method = "spearman")

# Mean Parent Years of Edu. & Score
year_edu_mean_parents <- train_data |>
  mutate(parent_edu_avg = (Medu + Fedu) / 2)

cor_test_mean_edu <- cor.test(year_edu_mean_parents$parent_edu_avg, year_edu_mean_parents$score, method = "spearman") 
```

Lastly, two dummy variables were created to capture the caregiver’s identity: `main_caregiver_mom` and `main_caregiver_dad`. Each takes the value 1 if the primary guardian is the specified caregiver, and 0 otherwise. These indicators may later be used in regression analyses as potential interaction terms to test whether the influence of parental education differs depending on which parent serves as the main caregiver.

```{r}

#| label: Dummy Variables for the Caregivers

# Create dummy variable main parent mom and main parent dad
train_linear <- train_data %>%
  mutate(
    main_caregiver_mom = case_when(
      guardian == "mother" ~ 1,
      guardian == "father" ~ 0,
      guardian == "other"  ~ 0
    ),
    main_caregiver_dad = case_when(
      guardian == "mother" ~ 0,
      guardian == "father" ~ 1,
      guardian == "other"  ~ 0
    )
  )

data_job_main_parent <- train_data %>%
  mutate(
    job_main_parent = case_when(
      guardian == "mother" ~ as.character(Mjob),
      guardian == "father" ~ as.character(Fjob),
      guardian == "other"  ~ "other"
    )
  )
```



```{r}
#| label: Check Interactions Family Educational Support (binary yes or no)

ggplot(data_year_edu_parent, aes(x = year_edu, y = score, fill = parent)) +
  geom_boxplot(outlier.alpha = 0.4, width = 0.6) +
  facet_wrap(~ famsup) + 
  labs(
    title = "Figure 22. Student Score vs Parental Education Level, by Family Support",
    x = "Years of Education",
    y = "Score",
    fill = "Parent"
  ) +
  theme_minimal()
```
# Model description

This study predicts student performance using three models: Linear Regression, k-Nearest Neighbours (KNN), and Random Forest. These models are used to identify the algorithm achieving the best predictive accuracy on the test set.

Linear Regression serves as the baseline model, providing interpretability and allowing estimation of both the direction and magnitude of relationships between predictors and exam scores. Stepwise selection methods (forward, backward, and mixed) were employed to identify the most informative subset of predictors while reducing redundancy.

K-Nearest Neighbours (KNN) provides a non-parametric alternative capable of modelling non-linear relationships. Its performance depends on the choice of k and on proper scaling of predictors, which was handled during the pre-processing stage.

Random Forest is an ensemble learning approach that combines multiple decision trees to improve predictive accuracy and mitigate overfitting. It can handle mixed variable types effectively and provides feature importance measures, enhancing both robustness and interpretability.
For each predictive algorithm, three versions of the model were trained and evaluated to assess how data representation and feature selection influenced performance. Each model was developed using the following versions of the dataset:

-Original Feature Model
This baseline model was trained using the dataset in its original structure without any feature reduction or transformation.


-Top-10 Correlation Model
This model was trained using only the ten predictors most strongly correlated with score, identified through 10-fold cross-validation. This procedure ensured that feature selection was data-driven and robust against random variation.


-Feature-Selection Model (Linear Regression only)
Linear regression was trained using a feature-selection encoding method that reduced dimensionality by retaining only the most informative variables while encoding categorical data. This approach balanced model simplicity and predictive power by minimizing redundancy among correlated predictors.


-One-hot encoding Model (KNN and Random Forest only). 
This model converts categorical data into a numerical format suitable for these algorithms, preventing the model from inferring any ordinal relationships between categories. This representation helps avoid bias and typically improves overall predictive performance.

Feature selection was used only to the linear regression model because linear models are highly sensitive to multicollinearity and the inclusion of irrelevant predictors. Feature selection mitigates these issues. 

In contrast, KNN and Random Forest are better at handling high-dimensional data and multicollinearity. Therefore, explicit feature selection was less critical for these algorithms. Thus, one-hot encoding was applied because both algorithms require purely numerical input, whereas linear regression handled the categorical variables through the feature-selection process.


Each model variant was evaluated using Mean Squared Error (MSE) and R² on both training and test sets. The best-performing variant for each algorithm was carried forward to the final model comparison.

This systematic design ensured that each algorithm was assessed fairly, accounting for differences in feature representation and predictor relevance, while mitigating redundancy and overfitting.

### Linear regression model

To refine the linear model, we first applied three stepwise selection approaches. First, forward selection which adds predictors sequentially based on model improvement. It starts with an empty model and adds predictors one by one. At each step, the variable that most improves model fit is included until no further improvement is achieved.

After that we used backward selection, which removes the least significant variables until only relevant ones remain. It begins with all predictors in the model and removes predictors that are least significant. 

Finally, we will used mixed selection (stepwise), this combines both approaches so it adds variables like forward selection, but it also removes variables when a variable is becomes non-significant when a new variable is added. This helps prevent that redundant variables are in the dataset. 


After, the stepwise selection approaches, we also did a selection by hand by first running a linear regression with all variables included after which we selected the significant variables. We also run the same regression including the interaction terms Father job x main_caregiver_dad,  and Mother job x main_caregiver_mom, the variables we created earlier in the pre-processing. 

We also made a regression model that includes the 10 variables that correlate the highest with score, as we saw in the correlation matrix. In a sepearte regression, we also added the interaction year education mom x main guardian mom, and the interaction year education dad x main guardian dad. 

Finally, we will compare the different linear models.


1. Method 1: Linear regression stepwise selection
```{r}
#| label: linear regression stepwise selection 

# Set seed for linear regression model
set.seed(2025)


train_control <- trainControl(method = "cv",number = 10, savePredictions = "final") #K-fold crossvalidation, set k= 10


# Linear regression with Backward selection 
model_lm_backward <- train(
  score ~ ., 
  data = train_linear,
  method = "lmStepAIC",
  trace = FALSE,
  direction = "backward",#set method to step backward 
  trControl = train_control)
#tidy(model_lm_backward$finalModel)

# Linear regresion with Forward selection 
model_lm_forward <- train(
  score ~ .,
  data = train_linear,
  method = "lmStepAIC", 
  direction = "forward", #set method to step forward 
  trace = FALSE,       # suppress stepwise printing
  trControl = train_control)
 #tidy(model_lm_forward$finalModel)


# Linear regression with Stepwise (both)
model_lm_both <- train(
  score ~ .,
  data = train_linear,
  method = "lmStepAIC", 
  direction = "both",#set method to step both 
  trace = FALSE,       # suppress stepwise printing
  trControl = train_control)
#tidy(model_lm_both$finalModel)

# Linear regression with all variables
regression_all_var <- train( 
  score ~ .,
  data = train_linear,
  method = "lm", # for regression
  trControl = train_control) 


#names(coef(model_lm_backward$finalModel))
#names(coef(model_lm_forward$finalModel))
#names(coef(model_lm_both$finalModel))
```


```{r}

# Compare models with original data
model_comparison_Linear <- data.frame(
  Model = c("Backward", "Forward", "Both", "All Variables"),
  RMSE = c(
    min(model_lm_backward$results$RMSE),
    min(model_lm_forward$results$RMSE),
    min(model_lm_both$results$RMSE),
    min(regression_all_var$results$RMSE)
  ),
  Rsquared = c(
    max(model_lm_backward$results$Rsquared),
    max(model_lm_forward$results$Rsquared),
    max(model_lm_both$results$Rsquared),
    max(regression_all_var$results$Rsquared)
  ),
  MAE = c(
    min(model_lm_backward$results$MAE),
    min(model_lm_forward$results$MAE),
    min(model_lm_both$results$MAE),
    min(regression_all_var$results$MAE)
  )
)

```


2. Method 2: significance feature selection + modelling
```{r}
#| label: regression add sig variable

sig_vars <- tidy(regression_all_var$finalModel) |>
  mutate(significance = case_when(
    p.value < 0.05 ~ "Significant (p < 0.05)",
    p.value < 0.10 ~ "Marginally significant (0.05 ≤ p < 0.10)",
    TRUE ~ "Not significant")) |>
  filter(p.value < 0.10) |>  # keep both significant and marginally significant
  arrange(p.value)


##regression only sign. predictors 
sig_var <- c("failures", "sex", "schoolsup", "goout", "studytime", "romantic", "Fjob", "age")   #select significant variables and marginally significant (p between 0.5-0.1)
        

formula_sig_var <- as.formula(paste("score ~", paste(sig_var, collapse = "+"))) #create formula 


##k-fold cross-validation
regression_sig_var <- train(
  formula_sig_var,
  data = train_linear,
  method = "lm", # for linear regression
  trControl = train_control)

#print(regression_sig_var)
#tidy(regression_sig_var$finalModel)


```



3. Method 3: Top 10 correlation feature selection + modeling
```{r}
#| label: regression top 10 
# Create copy for computing matrix
train_data_cor_matrix <- train_data

# Apply mapping into matrix data
for (col in names(binary_mapping)) {
  train_data_cor_matrix[[col]] <- as.numeric(binary_mapping[[col]][train_data_cor_matrix[[col]]])
}

# Correlation computation with K-fold CV 
col_all <- c(binary_cols, numeric_cols, numeric_ordinal,ordered_ordinal)
k <- 10  # number of folds

folds <- createFolds(train_data_cor_matrix$score, k = k, list = TRUE, returnTrain = TRUE)

fold_correlations <- lapply(folds, function(train_idx) {  # Store correlations for each k 
  fold_data <- train_data_cor_matrix[train_idx, ]
  sapply(fold_data[, col_all], function(x) cor(x, fold_data$score, method = "spearman"))
  })

cor_score <- rowMeans(do.call(cbind, fold_correlations), na.rm = TRUE) # Compute average correlation across folds

cor_df <- data.frame(   #Convert to data frame
  predictor = names(cor_score),
  correlation = cor_score)

# Plot correlation matrix 
ggplot(cor_df, aes(x = reorder(predictor, correlation), y = correlation)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(breaks = seq(-1, 1, by = 0.1)) +
  coord_flip() +
  theme_minimal() +
  labs(title = "Figure 23. Spearman Correlation with Score (mean of 10-Fold Cross-Validation)", x = "Predictor", y = "Spearman Correlation")
```


```{r}
#select top 10 highest correlations from correlation matrix 
top10_predictors <- cor_df |>
  filter(predictor != "score") |>           # filter score as this should not be predictor
  arrange(desc(abs(correlation))) |>        # sort by absolute correlation
  slice(1:10) |>                            # take top 10
  pull(predictor)  


formula_top10 <- as.formula(paste("score ~", paste(top10_predictors, collapse = "+"))) #make formula with top-10 predictors


regression_top10 <- train(
  formula_top10,
  data = train_linear,
  method = "lm", # for linear regression
  trControl = train_control)

#print(regression_top10)
```



4. Method 4: Modeling with top significant features + interaction
```{r}
#| label: linear regression sign + interaction 


###now same regression but interaction terms included 
interaction_terms <- c("Fjob:main_caregiver_dad", "Mjob:main_caregiver_mom") #the interaction terms 

# Build full formula including interactions
formula_sig_var <- as.formula(
  paste("score ~", paste(c(sig_var, interaction_terms), collapse = " + "))) #sign_var same list as above 

##k-fold cross-validation
regression_sig_var_interaction <- train(
  formula_sig_var,
  data = train_linear,
  method = "lm", # for linear regression
  trControl = train_control)
```


5. Method 5: Modelling with top 10 features + interaction
```{r}
#| label: top-10 plus interaction 

interaction_terms_top_10 <- c("Fjob:main_caregiver_dad", "Mjob:main_caregiver_mom")# Interaction terms

formula_top_10_interaction <- as.formula(
  paste("score ~", paste(c(top10_predictors, interaction_terms_top_10), collapse = " + ")) )# same formula as top-10, interactions added


# Train linear regression
regression_top_10_interaction <- train(
  formula_top_10_interaction,
  data = train_linear,
  method = "lm",
  trControl = train_control)

```

6. Comparing all the models
```{r}
#| label: compare linear models 


# combining linear regression  results
results_compare_linear_models <- resamples(list(
  Top10             = regression_top10,
  Top10_interaction = regression_top_10_interaction,
  SigVar            = regression_sig_var,
  SigVar_Interact   = regression_sig_var_interaction,
  All_Variables     = regression_all_var,
  Step_Backward     = model_lm_backward,
  Step_Forward      = model_lm_forward,
  Step_Both         = model_lm_both))



# Summarize results linear regression 

# Visualize RMSE comparison
#bwplot(results_compare_linear_models, metric = "RMSE")

# Visualize R-squared
bwplot(results_compare_linear_models, metric = "Rsquared",main = "Figure 24. Rsquared score across linear models")


###visualise MSE because that is our performance measure 
# Extract RMSE values
linear_rmse <- results_compare_linear_models$values[, grep("RMSE", names(results_compare_linear_models$values))]

# Compute MSE from RMSE
linear_mse_vals <- linear_rmse^2

# Melt into long format
linear_mse_df <- reshape2::melt(linear_mse_vals, variable.name = "Model", value.name = "MSE")

# Clean up model names (remove "~RMSE")
linear_mse_df$Model <- gsub("~RMSE", "", linear_mse_df$Model)

# Visualize using boxplot
ggplot(linear_mse_df, aes(x = Model, y = MSE)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Figure 25. Linear Regression Model Comparison", y = "Mean Squared Error", x = "Model") +
  theme_minimal()
    
```


### KNN 

In this analysis, a weighted K Nearest Neighbors (KNN) regression model was trained to predict students’ scores based on multiple predictors. The main goal was to compare different feature selection methods and see how they affect the model’s predictive performance.

First, we set a random seed (2025) to make the results reproducible and defined a 10-fold cross-validation setup using trainControl() from the caret package. This means the dataset was split into 10 parts: in each iteration, 9 folds were used for training and 1 for validation. This approach helps to get a more stable estimate of model performance and reduces overfitting compared to a single train/test split.

The first KNN model (model_knn_all) was trained using all available predictors. The tuneGrid defined the range of hyperparameters to explore; the number of neighbors (kmax), distance metrics and kernel functions. The model then automatically found the best combination of these parameters using cross-validation.

After fitting the full model, we calculated feature importance with varImp(). This step identifies which predictors had the most influence on the models output. The top ten most important features were selected and used to train a second KNN model (model_knn_top10_importance). This approach is similar to how significance-based selection is done in linear regression, focusing on the most influential predictors to simplify the model without losing too much predictive power.

Next, we applied a second feature selection method based on Spearman correlation. Each predictor’s correlation with the target variable (score) was computed using 10-fold cross-validation again and the top 10 most correlated predictors were selected. These were then used to train a third KNN model (model_knn_top10_corr).


1. KNN model with original data
```{r}

#target vars
y <- train_data_encode$score
x <- train_data_encode |> dplyr::select(-score)

###set seed and crossvalidation setup

colnames(train_data_encode) <- make.names(colnames(train_data_encode))

set.seed(2025)

tune_grid <- expand.grid(
  kmax = seq(1, 31, 2),  
  distance = c(1, 2),     
  kernel = c("rectangular", "triangular", "gaussian")
)


#KNN model with all vars
model_knn_all <- train(
  score ~ .,
  data = train_data_encode,
  method = "kknn",
  trControl = train_control,
  preProcess = c("center", "scale"), 
  tuneGrid = tune_grid
  
)
```

2. Finding the most important features
```{r}
#compute feature importance
varImp_knn <- varImp(model_knn_all, scale = TRUE)

#convert to tibble & select top 10 
top_features_importance <- varImp_knn$importance |>
  tibble::as_tibble(rownames = "feature") |>
  dplyr::arrange(desc(Overall)) |>
  dplyr::slice(1:10)



#categorize importance levels (analogous to signficance)
importance_df <- varImp_knn$importance |>
  tibble::as_tibble(rownames="feature") |>
  dplyr::mutate(importance_level=dplyr::case_when(
    Overall >= 0.75 * max(Overall, na.rm=TRUE) ~ "highly important (more or same as 75%)",
    Overall >= 0.50 * max(Overall, na.rm=TRUE) ~ "moderately important (50-75%)",
    Overall >= 0.25 * max(Overall, na.rm=TRUE) ~ "low importance (25-50%)",
    TRUE ~ "negligible importance (<25%)"
  )) |>
  dplyr::arrange(desc(Overall))



#plotting feature importance
ggplot(top_features_importance, aes(x = reorder(feature, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "#FF91A4") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Figure 26. Top 10 feature importances",
       x = "Feature", y = "Importance")
```

3. KNN model with top 10 features from the feature selection
```{r}
#ensuring feature names are syntactically valid
top10_importance_features <- make.names(top_features_importance$feature)

#building safe formula
formula_top10_importance <- as.formula(paste("score ~", paste(top10_importance_features, collapse = "+")))

#training the model
model_knn_top10_importance <- train(
  formula_top10_importance,
  data = train_data_encode,
  method = "kknn",
  trControl = train_control,
  preProcess = c("center", "scale"), 
  tuneGrid = tune_grid
)

```

4. Correlation-based feature selection (Spearman)
```{r}
#make copy and ensure all predictors are numeric for correlation
train_data_cor_matrix2 <- train_data_encode
train_data_cor_matrix2[, setdiff(names(train_data_cor_matrix2), "score")] <-
  lapply(train_data_cor_matrix2[, setdiff(names(train_data_cor_matrix2), "score")], function(col) {
    if (is.factor(col) || is.character(col)) {
      as.numeric(col)
    } else {
      col
    }
  })

#define predictors and folds
col_all2 <- setdiff(names(train_data_cor_matrix2), "score")
k <- 10
folds2 <- createFolds(train_data_cor_matrix2$score, k = k, list = TRUE, returnTrain = TRUE)

#compute Spearman correlations
fold_correlations2 <- lapply(folds2, function(train_idx) {
  fold_data2 <- train_data_cor_matrix2[train_idx, ]
  sapply(fold_data2[, col_all2], function(x) cor(as.numeric(x), fold_data2$score, method = "spearman"))
})


#average correlations across folds
cor_score2 <- rowMeans(do.call(cbind, fold_correlations2), na.rm = TRUE)

#converting to data frame
cor_df2 <- data.frame(
  predictor = names(cor_score2),
  correlation = cor_score2
)

#select top 10 predictors
top10_predictors <- cor_df2 |>
  tibble::as_tibble() |>                        #esure it's a tibble bc it gave an error with df
  dplyr::filter(predictor != "score") |>        # filter out target variable
  dplyr::arrange(desc(abs(correlation))) |>     #sort by absolute correlation
  dplyr::slice(1:10) |>                         #explicitly call dplyr's slice
  dplyr::pull(predictor)


print(top10_predictors)
```

5. KNN model with top10 corr predictors
```{r}
formula_top10_corr <- as.formula(paste("score ~", paste(top10_predictors, collapse = "+")))

model_knn_top10_corr <- train(
  formula_top10_corr,
  data = train_data_encode,
  method = "kknn",
  trControl = train_control,
  preProcess = c("center", "scale"), 
  tuneGrid = tune_grid
)

```

6. Compare KNN models (all vars, importance, corr)

```{r}
results_compare_KNN <- resamples(list(
  All_Variables = model_knn_all,
  Top10_Importance = model_knn_top10_importance,
  Top10_Correlation = model_knn_top10_corr
))

bwplot(results_compare_KNN, metric = "Rsquared",main = "Figure 27. Rsquared score across linear models")
bwplot(results_compare_KNN, metric = "RMSE", main = "Figure 28. Rsquared score across linear models")
```

### Random Forest model

1. Concept
Random Forest is a decision tree based model which combines multiple decision trees to improve prediction accuracy and stability. It will first create multiple training subsets (bootstrap samples), then for each sample, one decision tree is created. At split node, at least two features will be considered. It will stop when trees reach to max depth or satisfy certain condition. 

2. Implementation
Here we use 'caret' which is a R package that provides algorithms include Random Forest, data preprocessing, training, evaluation, also cross-validation and hyperparameter tuning tools to build our Random Forest model.

In our implementation:
- Set tunelength = 6 to try different mtry values
- Set number of tree as 500
- 10 fold cross-validation

3. Model Performance

Three models is generated for Random Forest regression. The first one is trained on original data, the best result is mtry=16(The model considers 16 features) with RMSE=0.8529151 and Rsquared=0.2569956. The second one is implemented on encoded data with the same parameter, the best result is mtry=62 with RMSE=0.8874719 and Rsquared=0.1952404. For the last model, we implement model with feature selection on original data which give the best result of mtry=2, RMSE=0.8752171 and Rsquared= 0.2151779.

4. Advantages of Random Forest

First, Random Forest can deal with redundant data automatically and does not need to do correlation to select feature. Secondly, it is able to discover interactions between features. Therefore, there is not preprocessing of feature engineering in Random Forest part. Finally, it is able to reduce overfitting by ensembling trees. 

5. Questions with Random Forest

- Are parameters tunelength = 6, ntree =500 enough?  
*tunelength* in caret automatically defines numbers of *mrty* are tested when we did not specify *tuneGrid*. Random Forest is not sensitive with small changes of mtry, therefore, trying around six representative values across possible range is sufficient to find the optimal. *ntree* is the number of tree we want Random Forest to generate, 500 is a common and stable choice for sample size of our data.

- Why do model perform worse after encoding? feature selection simplify the model, does it justify the accuracy trade off?  
The structure of the data is change after encoded. For instance, sex.M - (1,0) and sex.F - (0,1), one column is turned into two, which makes each variables less information, and it is hard for Random Forest to find a complete signal.

- mtry is higher than 30, why?  
When the tree is split, more features are considered. For instance, sex would be split into sex.M and sex.F.


- What strategy can be used to improve?  
Hyperparameter tuning may be used to improve by setting *tuneGrid*, increase numbers of tree or mtry. Also setting up tree depth with *maxnodes* might prevent overfitting.



1. Basic RF model
```{r}
# Set seed for Random Forest Model
set.seed(2025)

#tunelength is number of mtry. mtry is number of features when a tree is created.

rf_model6 <- train(
  score ~ ., 
  data = train_data, 
  method = "rf", 
  trControl = train_control, #method is train_control
  tuneLength = 6,
  ntree=500
)
```


```{r}
# Check variables importance and plot top 10 features
importance5 <- varImp(rf_model6)
plot(importance5,top = 10,main="Figure 29. Top 10 features")
```


2. Model with encoded data
```{r}
rf_model_encode <- train(
  score ~ ., 
  data = train_data_encode, 
  method = "rf", 
  trControl = train_control, 
  tuneLength = 6,
  ntree=500
)
```



3. Modeling with Feature selection 
- top 10 It is better than just top 10 because original top 10 might be the features that are not relevant.
```{r}
# Get all features with importance
imp_data <- varImp(rf_model6)$importance
imp_data$Variable <- rownames(imp_data)
imp_data <- imp_data[order(-imp_data$Overall), ]

fea_select <- function(imp_data, top_n = 10) {
  # important feature for education
  domain_val <- c("failures", "studytime", "absences", "Medu", "Fedu", "higher")
  
  # top 10 feature
  top_important <- head(imp_data$Variable, top_n)
  
  # combine and remove duplicate
  final_val <- unique(c(top_important, domain_val))
  
  # can only get 12 variables
  final_val <- final_val[1:min(10, length(final_val))]
  
  return(final_val)
}

final_selection <- fea_select(imp_data, top_n = 10)
```

```{r}
top10 <- c("failures","absences","goout","Medu","health","Walc","age","Fedu","freetime","studytime")

formula_top10 <- as.formula(paste("score ~", paste(top10, collapse = "+")))
```


```{r}
#train top10's model
rf_top10 <- train(
  formula_top10,
  data = train_data,
  method = "rf",
  trControl = train_control,
  tuneLength = 6,
  ntree=500
)
```
4. Compare between random forest models
```{r}
results_compare_rf <- resamples(list(
  RF = rf_model6,
  RF_Encoded= rf_model_encode,
  Top10 = rf_top10
))

bwplot(results_compare_rf, metric = "Rsquared",main = "Figure 30. Random Forest Comparison")
```

# Model comparison
Within each model type (K-Nearest Neighbour (KNN), Linear regression and Random Forest), the optimal model was selected based on cross-validation performance. For random forest, mtry = 16 has the best result (rf_model6). For KNN, the optimal model is kmax = 21, distance = 2 and kernel = gaussian (model_knn_top10_corr). Finally, for the linear regression, it is the model with the lowest Mean Squared Error (MSE), which is model regression_top10. 
We then compared the best models with each other, using the 10-fold cross-validation. 

Looking at figures 31, 32 and 32, we compare the root mean square error (RMSE), R-squared and mean absolute error (MAE) of each model. For the RMSE, Random Forest obtains a RMSE of 0.8529151, while KNN obtains a RMSE of 0.8710617 and linear regression obtains a RMSE of 0.8749449. Since a lower RMSE means a better model, Random Forest comes first, followed by KNN and Linear Regression. 
The same goes for the MAE. Random forest achieves an MAE of 0.6577222, while KNN achieves an MAE of 0.6893002, and Linear Regression achieves an MAE of 0.6822717. Since lower values mean a better model, Random Forest comes first, followed by Linear Regression and KNN. 

When it comes to R-squared, Random Forest obtains an R-squared of 0.2569956, KNN obtains an R-squared of 0.2384261, and Linear regression obtains an R-squared of 0.2322802. Since a value closer to 1 means a better mode, Random Forest performs better, followed by KNN and Linear Regression. But, looking at Figure 32, Linear Regression seems to have a higher R-squared value than KNN. It is due to the fact that the boxplot shows the median, while the summary numbers show the mean of all 10 folds. It means that KNN R² means is higher, but its median is lower than Linear Regression. However, it is not really relevant for our analysis, as we are looking at the MSE score to select the best model. 

```{r}
# Create results comparison
results <- resamples(list(
  LinearRegression = regression_top10,
  KNN = model_knn_top10_corr,
  RandomForest = rf_model6
))

# Create RMSE and R-squared plots
bwplot(results, metric = "RMSE", main = "Figure 31. RMSE score across model")
bwplot(results, metric = "Rsquared", main = "Figure 32. Rsquared score across model ")
bwplot(results, metric = "MAE", main = "Figure 33. MAE score across model ")

# Extract RMSE values for MSE calculation
rmse_vals <- results$values[, grep("RMSE", names(results$values))]

# Compute MSE from RMSE
mse_vals <- rmse_vals^2
mse_df <- melt(mse_vals, variable.name = "Model", value.name = "MSE")

# Clean up model names
names(mse_df) <- c("Model", "MSE")
mse_df$Model <- gsub("~RMSE", "", mse_df$Model)
```


# Chosen model

When comparing the MSE score between models, we see that Random Forest is the most performant one, as its MSE is 0.7274642 (a lower number means a better performance). KNN’s MSE is 0.7587485 and Linear Regression’s MSE is 0.7595943, therefore, we consider these two models are having similar performance. 

```{r}
rmse_best <- c(
  LinearRegression = 0.8715471,
  KNN = 0.8710617,
  RandomForest = 0.8529151
)

mse_best <- rmse_best^2



data.frame(
  Model = names(mse_best),
  MSE = as.numeric(mse_best),
  row.names = NULL
)
```

# Prediction creation

```{r}

# Make predictions using the best model (Random Forest)
predictions <- predict(rf_model6, newdata = test_data)

# Store predictions
write_rds(predictions, "predictions.rds")
```

# Team member contributions
-   Menno: Wrote the first half of the report (up to the model comparisons)
-   Ilse: Linear regression model, data pre-processing, visualisation.
-   Zoé Ricardie: heatmap (data visualisation), model comparison, chosen model and prediction creation.
-   Majdouline: KNN model, visualisation. 
-   Zexuan: Random Forest model, data pre-processing, combined works together into reports.



# References

Erola et al (2016):
https://doi.org/10.1016/j.rssm.2016.01.003

Komorowski et al. (2016)
https://doi.org/10.1007/978-3-319-43742-2_15


Rakesh et al (2024):
https://doi.org/10.1111/jcpp.14082

Munir et al (2023):
https://doi.org/10.54183/jssr.v3i2.308

Yuen et al (2023)
https://doi.org/10.3389/fpsyg.2023.1253842







Erola, J., Jalonen, S., & Lehti, H. (2016). Parental education, class and income over the early life course and children’s achievement. Research in Social Stratification and Mobility, 44, 33–43. https://doi.org/10.1016/j.rssm.2016.01.003

Komorowski, M., Marshall, D. C., Salciccioli, J. D., & Crutain, Y. (2016). Exploratory data analysis. In M. M. Ghassemi, M. Naumann, S. J. Shah, & G. Szolovits (Eds.), Secondary Analysis of Electronic Health Records (pp. 185–203). Springer. https://doi.org/10.1007/978-3-319-43742-2_15

Rakesh, D., Lee, P. A., Gaikwad, A., & McLaughlin, K. A. (2025). Associations of socioeconomic status with cognitive function, language ability, and academic achievement in youth: A systematic review of mechanisms and protective factors. Journal of Child Psychology and Psychiatry, 66(4), 417–439. https://doi.org/10.1111/jcpp.14082

Munir, J., Faiza, M., Jamal, B., Daud, S., & Iqbal, K. (2023). The impact of socio-economic status on academic achievement. Journal of Social Sciences Review, 3(2), 695–705. https://doi.org/10.54183/jssr.v3i2.308

Yuen, C. Y. M., Cheung, A. C. K., & Leung, K. H. (2023). Effects of salient factors on the pursuit of higher education among multicultural youth in Hong Kong. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1253842
